---
title: "Prediction of malaria deaths in Sub-Saharan Africa"
author: "Franz-Xaver Wienerroither  and Lucas Stadler"
format:
  revealjs: 
    theme: default
editor_options: 
  chunk_output_type: console

---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
set.seed(123)
```

<!-- Read and build y and X -->

```{r, Raw form}
rm(list=ls())

setwd("~/Documents/Studium/MAKRO1/disease-prediction-bayes")
dat <- read.csv("data/processed/final_table_avg.csv")

Y_raw <- dat$Estimate.y                         # 38 × 1
X_raw <- cbind(
  1,
  dat$GDP.per.capita..PPP..constant.2021.international...,
  dat$Current.health.expenditure.per.capita..PPP..current.international...,
  dat$rate.over65,
  dat$Estimate.x,
  dat$Population.density,
  dat$Estimated.mortality.from.all.forms.of.tuberculosis.per.100.000.population,
  dat$Estimated.number.of.deaths.from.all.forms.of.tuberculosis,
  dat$Share.of.population.residing.in.urban.areas..HYDE.estimates.and.UN.projections.
)
colnames(X_raw) <- c("Intercept","GDP","Health","Pop65",
                     "MalariaInc","Density","TB_Mort",
                     "TB_Deaths","Urban")
sum(is.na(X_raw))

# Mean-impute the single NA in Health
X_raw[is.na(X_raw[,"Health"]),"Health"] <-
  mean(X_raw[,"Health"], na.rm = TRUE)

```

```{r, Normalized}
Y <- as.matrix(scale(dat$Estimate.y))       # response: malaria mortality (38 × 1)

X <- cbind(1,                                   # intercept
           scale(dat$GDP.per.capita..PPP..constant.2021.international...),
           scale(dat$Current.health.expenditure.per.capita..PPP..current.international...),
           scale(dat$rate.over65),
           scale(dat$Estimate.x),                # malaria incidence
           scale(dat$Population.density),
           scale(dat$Estimated.mortality.from.all.forms.of.tuberculosis.per.100.000.population),
           scale(dat$Estimated.number.of.deaths.from.all.forms.of.tuberculosis),
           scale(dat$Share.of.population.residing.in.urban.areas..HYDE.estimates.and.UN.projections.))

# scale normalizes all the predictor variables so they have mean 0 and sd 1.
# important: if we want to interpret the results for the prediction later, we need to descale again

# rename columns
colnames(X) <- c("Intercept","GDP","Health","Pop65","MalariaInc",
                 "Density","TB_Mort","TB_Deaths","Urban")

# Mean-impute the single NA in Health
X[is.na(X[,"Health"]),"Health"] <-
  mean(X[,"Health"], na.rm = TRUE)
```

## Dataset

- Source: World Bank, World development index
- Data is normalized

```{r}
print("Raw data")
head(round(X_raw[,-1],2))
print("Normalized data")
head(round(X[,-1],2))
```

## Handling of missing values

One missing values for health expenditures gets replaced by mean imputation to assure computeability.

## Multicolliniarity of the predictors

```{r}
cor_matrix <- cor(X[,-1])  # Exclude the intercept
print(round(cor_matrix, 2))  # Rounded for readability
```

-   Nothing over 0.8 $\rightarrow$ looks good
-   Highest correlation between GDP and Health expenditures (0.69)

## Linear model for malaria death's

CAVE: Interpretability due to scaling limited

```{r}
library(car)

M_raw <-data.frame(Y = Y_raw,
                    X_raw
                )

lm_model <- lm(Y ~ .-Intercept, M_raw)  # Exclude intercept manually
summary(lm_model)


M_scale <- data.frame(Y = Y,
                  X
                  )

lm_model_scale <- lm(Y ~ .-Intercept, M_scale)
summary(lm_model_scale)
```

## Variance Inflation Factor 
is at max \~2.43 $\rightarrow$ moderate correlation

```{r}
vif(lm_model) # nothing over 5, looks good
vif(lm_model_scale)
```

# SSVS model build

## Preconditions

We simply took those from the original script:

```{r, echo=T}
nsave <- 10000
nburn <- 2000
ntot  <- nsave + nburn        # total Gibbs draws

tau0 <- 0.01                  # SSVS hyper-pars
tau1 <- 10
s0   <- 0.01                  # IG(a,b) prior on σ²
S0   <- 0.01
```

Section can stay the same, because we use same object names

```{r, echo=T}
N <- nrow(X)            # number of countries
K <- ncol(X)            # number of predictors

# Solve creates inverse so we have here (X'X)^-1 (X'Y) = \hat{\beta}.
# result is Matrix of OLS regression coefficients
A.OLS <- solve(crossprod(X))%*%crossprod(X,Y)

# analogous (Y-X\beta)'(Y-X\beta)
SSE <- crossprod(Y-X%*%A.OLS) 

#Estimate of error variance, N number observations, K number predictors
SIG.OLS <- SSE/(N-K) 
```

## Storage matrices for Gibbs loop and initialization of prior

```{r, echo=T}
# indicators, start with full model
# we want to start with all predictors included.
# so we create a Kx1 Matrix filled with 1
gamma <- matrix(1,K,1) 

# converts to numeric if it wasn't already
sigma2.draw <- as.numeric(SIG.OLS)

# picks either tau1 or tau0 as prior variance for the variable
V.prior <- diag(as.numeric(gamma*tau1+(1-gamma)*tau0)) 

# creates matrix with NA entries, nsave rows, K columns
ALPHA.store <- matrix(NA,nsave,K) 

# creates matrix with NA entries, nsave rows, 1 columns
SIGMA.store <- matrix(NA,nsave,1)

# creates matrix with NA entries, nsave rows, K columns
Gamma.store <- matrix(NA,nsave,K) 
```

## Gibbs loop

```{r, echo=T, results=F}
for (irep in 1:ntot){
  # Draw ALPHA given rest from multivariate normal
  # we need to create the meand and variances to draw 
  # from the distribution ntot times
  
  # Constructing posterior variance according to multivariate formula 
  # V.post = (1/\sigma^2 * X'X + V^-1.prior)^-1
  V.post <- solve(crossprod(X)*1/sigma2.draw+diag(1/diag(V.prior)))
  
  # Computes the posterior mean using the formula
  A.post <- V.post%*%(crossprod(X,Y)*1/sigma2.draw)
  
  # Draws sample from multivariate normal distribution
  A.draw <- A.post+t(chol(V.post))%*%rnorm(K)
  
  #Draw indicators conditional on ALPHA
  for (jj in 1:K){
    # Probability under "exclude" prior
    p0 <- dnorm(A.draw[[jj]], 0, sqrt(tau0))
    
    # Probability under "include" prior
    p1 <- dnorm(A.draw[[jj]], 0, sqrt(tau1)) 
    
    # Posterior inclusion probability
    p11 <- p1/(p0+p1) 
    
    # Bernoulli draw
    if (p11>runif(1)) gamma[[jj]] <- 1 else gamma[[jj]] <- 0 
  }
  # Construct prior VC matrix conditional on gamma
  V.prior <- diag(as.numeric(gamma*tau1+(1-gamma)*tau0))
  
  #Simulate sigma2 from inverse Gamma
  S.post <- crossprod(Y-X%*%A.draw)/2+S0
  s.post <- S0+N/2
  sigma2.draw <- 1/rgamma(1,s.post,S.post)  
  
  if (irep>nburn){
    ALPHA.store[irep-nburn,] <- A.draw
    SIGMA.store[irep-nburn,] <- sigma2.draw
    Gamma.store[irep-nburn,] <- gamma
  }
  print(irep)
}
```

## Calculate posterior inclusion probabilities

```{r, echo=T}
PIP.mean <- apply(Gamma.store,2,mean)
A.mean   <- apply(ALPHA.store,2,mean)
SIG.mean <- apply(SIGMA.store,2,mean)
```

## SSVS Trace-Plot

```{r}
par(mfrow=c(3,3))  # arrange plots

for (k in 1:K){
  plot(ALPHA.store[,k], type='l',
       main=colnames(X)[k],
       xlab="Iterations", ylab="Coefficient")
}
```

## Sigma² Trace

```{r}
par(mfrow=c(2,2))
plot(SIGMA.store, type='l', main="Sigma² Trace", xlab="Iteration", ylab="Variance")
acf(SIGMA.store, main = "Series")
hist(SIGMA.store, breaks=50, main="Sigma² Histogram")
```

## Combine and display

```{r}
importance <- data.frame(Variable = colnames(X),
                         PIP = round(PIP.mean, 3),
                         PostMean = round(A.mean, 3))

print(importance[order(-importance$PIP), ])  # Sort by importance
```

## Posterior density

```{r}
par(mfrow=c(3,3))
for (k in 1:K){
  hist(ALPHA.store[,k], main=colnames(X)[k],
       xlab="Posterior Draws", breaks=30, col="skyblue", probability = T)
  lines(density(ALPHA.store[,k]), col="red")
}
```

## Summary Alpha
```{r}
colnames(ALPHA.store) <- colnames(X)
summary(ALPHA.store)
```

## Summary Sigma
```{r}
colnames(SIGMA.store) <- "Sigma"
summary(SIGMA.store)
```

## Summary Gamma
```{r}
colnames(Gamma.store) <- colnames(X)
summary(Gamma.store)
```

## Posterior Inclusion Probabilities
```{r}
par(mfrow=c(1,1))
barplot(PIP.mean, names.arg=colnames(X), las=2, col="skyblue",
        main="", ylim=c(0,1))
abline(h=0.5, col="red", lty=2)
```

# Leave-one-out


```{r, include=FALSE}
# -----------------  SSVS – Leave-One-Out CV  -----------------
library(MASS)    # mvrnorm() fallback if chol fails

# -------  scale X (except intercept) & Y  ----------------
X_scaled <- X_raw
X_scaled[,-1] <- scale(X_raw[,-1])
Y_scaled <- scale(Y_raw)
Y_mean   <- attr(Y_scaled, "scaled:center")
Y_sd     <- attr(Y_scaled, "scaled:scale")
```

## Hyperparameters
```{r}
# ---------- 2. Hyper-parameters & storage -------------------------------
nsave <- 1000
nburn <- 1000
ntot  <- nsave + nburn

tau0 <- 0.1          # << relaxed spike
tau1 <- 10
s0   <- 0.01
S0   <- 0.01

N <- nrow(X_scaled)
K <- ncol(X_scaled)

pred_scaled <- rep(NA, N)
pip_mat     <- matrix(NA, N, K)   # PIP per left-out fold
```

## LOO
```{r}
# ---------- 3. LOO loop --------------------------------------------------
for (i in 1:N) {
  cat("LOO fold", i, "of", N, "\n")
  
  X_train <- X_scaled[-i, ]
  Y_train <- Y_scaled[-i]
  X_test  <- X_scaled[i, , drop = FALSE]
  
  # ----- OLS starting values ----------
  A.OLS <- solve(crossprod(X_train)) %*% crossprod(X_train, Y_train)
  SSE   <- crossprod(Y_train - X_train %*% A.OLS)
  SIG.OLS <- SSE / (N-1 - K)
  
  gamma <- matrix(1, K, 1)
  sigma2.draw <- as.numeric(SIG.OLS)
  V.prior <- diag(as.numeric(gamma * tau1 + (1-gamma) * tau0))
  
  # storage for this fold
  alpha_keep  <- matrix(NA, nsave, K)
  gamma_keep  <- matrix(NA, nsave, K)
  sigma_keep  <- numeric(nsave)
  y_pred_keep <- numeric(nsave)
# ------------- Gibbs sampler -----------------
  for (rep in 1:ntot){
    
    # 1. β | rest
    V.post <- solve(crossprod(X_train) / sigma2.draw +
                      diag(1 / diag(V.prior)))
    A.post <- V.post %*% (crossprod(X_train, Y_train) / sigma2.draw)
    A.draw <- A.post + t(chol(V.post)) %*% rnorm(K)
    
    # 2. γ | rest
    for (j in 1:K){
      p0 <- dnorm(A.draw[j], 0, sqrt(tau0))
      p1 <- dnorm(A.draw[j], 0, sqrt(tau1))
      gamma[j] <- rbinom(1, 1, p1/(p0+p1))
    }
    V.prior <- diag(as.numeric(gamma * tau1 + (1-gamma) * tau0))
    
    # 3. σ² | rest
    S.post <- crossprod(Y_train - X_train %*% A.draw)/2 + S0
    s.post <- S0 + (N-1)/2
    sigma2.draw <- 1 / rgamma(1, s.post, S.post)
    
    # ---- store after burn-in
    if (rep > nburn) {
      m <- rep - nburn
      alpha_keep[m, ]  <- A.draw
      gamma_keep[m, ]  <- gamma
      sigma_keep[m]    <- sigma2.draw
      y_pred_keep[m]   <- X_test %*% A.draw                # mean prediction
    }
  } # end Gibbs

# Posterior predictive mean for left-out obs
  pred_scaled[i] <- mean(y_pred_keep)
  pip_mat[i, ]   <- colMeans(gamma_keep)
}
```


```{r}
# ---------- 4. Back-transform predictions & evaluate  --------------------
pred_original <- pred_scaled * Y_sd + Y_mean

loo_results <- data.frame(
  Country   = dat$Entity.x,
  True      = Y_raw,
  Predicted = pred_original,
  Error     = pred_original - Y_raw,
  MSE = (pred_original - Y_raw)^2
)

print(head(loo_results, 10))
cat("RMSE: ", sqrt(mean(loo_results$Error^2)), "\n")
```


```{r}
# ---------- 5. Average PIPs across folds --------------------------------
overall_pip <- colMeans(pip_mat)
print(round(overall_pip, 3))
barplot(overall_pip, names.arg=colnames(X_scaled), las=2, ylim=c(0,1),
        col="skyblue", main="Avg PIP across LOO folds")
abline(h=0.5, col="red", lty=2)
```


```{r, eval=FALSE}
# ---------- 6. Save results ---------------------------------------------
dir.create("results", showWarnings = FALSE)
write.csv(loo_results, "results/loo_predictions.csv", row.names = FALSE)
```






